{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide title"
        }
      },
      "source": [
        "# 2. Working with Text Data\n",
        "\n",
        "> LLMs from Scratch\n",
        "\n",
        "- Minjae Gwon\n",
        "  <minjae.gwon@postech.ac.kr>\n",
        "  <https://bxta.kr>\n",
        "\n",
        "- ML Lab\n",
        "  <https://ml.postech.ac.kr>\n",
        "\n",
        "- CompSec Lab\n",
        "  <https://compsec.postech.ac.kr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide subtitle"
        }
      },
      "source": [
        "## 2.1 Understanding Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "header"
        }
      },
      "source": [
        "### Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide body"
        }
      },
      "source": [
        "- 데이터를 벡터의 형태로 변환하는 것\n",
        "  - 텍스트, 이미지 등의 이산 데이터를 continuous vector space에 올리는 매핑\n",
        "- LLM을 포함한 deep neural network들은 텍스트를 바로 처리할 수 없기 때문에 필요"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "header"
        }
      },
      "source": [
        "### Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide body"
        }
      },
      "source": [
        "- Example: *Word2Vec*\n",
        "  - 가정: 비슷한 의미를 가진 단어들은 비슷한 벡터로 매핑됨\n",
        "  - 2차원 공간에 word embedding을 시각화하면 비슷한 의미를 가진 단어들이 서로 가까이 위치함\n",
        "    - ![](https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/03.webp)\n",
        "- Real World: LLMs\n",
        "  - 자기 자신만의 embedding layer를 input layers 중 하나로 구성하고 학습 단계에 업데이트 하는 것이 일반적\n",
        "    - 즉, pretrained *Word2Vec* 모델을 이용하지 않음\n",
        "  - 특정 task와 data에 optimized 된 embedding을 얻을 수 있음\n",
        "  - 2차원보다 훨씬 큰 차원의 embedding을 이용함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide subtitle"
        }
      },
      "source": [
        "## 2.2 Tokenizing Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "header"
        }
      },
      "source": [
        "### Goal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "body"
        }
      },
      "source": [
        "- 20,479개의 글자로 이루어진 짧은 이야기 \"The Verdict\"를 tokenize 하는 것"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Exercise: Load Document\n",
        "\n",
        "- \"The Verdict\"를 `document` 변수에 저장하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "slideshow": {
          "slide_type": "slide body"
        }
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "from llm_from_scratch.chapter_02.tests import test_document_loaded_correctly\n",
        "\n",
        "path_of_document = Path(\"the-verdict.txt\")\n",
        "\n",
        "document: str = ...\n",
        "\n",
        "test_document_loaded_correctly(document)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of character: 20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ],
      "source": [
        "print(\"Total number of character:\", len(document))\n",
        "print(document[:99])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "header"
        }
      },
      "source": [
        "### Simple Approach: Word-Based Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "slideshow": {
          "slide_type": "slide body"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "text = \"Hello, world. This, is a test.\"\n",
        "result = re.split(r\"(\\s)\", text)\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "body"
        }
      },
      "source": [
        "- 주어진 텍스트를 토큰의 리스트로 변환하기 위해 `re.split`을 사용할 수 있음\n",
        "  - `re.split`은 정규표현식을 이용해 텍스트를 나누는 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "slideshow": {
          "slide_type": "slide body"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
          ]
        }
      ],
      "source": [
        "text = \"Hello, world. Is this-- a test?\"\n",
        "\n",
        "splitted = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "result = [item.strip() for item in splitted if item.strip()]\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "body"
        }
      },
      "source": [
        "- 문장 부호들과 같은 특수 문자들을 분리함\n",
        "- 예시의 간결함을 유지하기 위해 whitespace를 삭제\n",
        "  - 경우에 따라 whitespace를 유지하는 것이 이로울 수도 있음\n",
        "    - e.g. Python Code (indentation 때문에)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Exercise: Tokenize Document\n",
        "\n",
        "- `re.split`을 이용해 위와 같은 규칙으로 `document`를 tokenize해보자."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "slideshow": {
          "slide_type": "slide body"
        }
      },
      "outputs": [],
      "source": [
        "from llm_from_scratch.chapter_02.tests import test_document_tokenized_correctly\n",
        "\n",
        "tokenized_document: list[str] = ...\n",
        "\n",
        "test_document_tokenized_correctly(tokenized_document)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of `preprocessed_the_verdict`: 4690\n",
            "Samples:  ['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
          ]
        }
      ],
      "source": [
        "print(\"Length of `preprocessed_the_verdict`:\", len(tokenized_document))\n",
        "print(\"Samples: \", tokenized_document[:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.3 Converting Tokens into Token IDs\n",
        "\n",
        "### Goal\n",
        "\n",
        "- 각 토큰을 고유한 정수의 ID로 변환하기\n",
        "  - Embedding vector로 변환하기 위해 필요\n",
        "\n",
        "### Vocabulary\n",
        "\n",
        "- 각 토큰과 특수 문자에 대한 고유한 ID의 mapping\n",
        "  - ![](https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/06.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Exercise: Create Vocabulary\n",
        "\n",
        "- 앞서 생성한 `tokenized_document`를 이용해 vocabulary를 생성해보자.\n",
        "- `vocabulary`는 토큰에 대해 고유한 숫자를 매핑하는 Python dictionary여야 함\n",
        "- Alphabetical order로 정렬하되 중복을 제거해야 함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llm_from_scratch.chapter_02.tests import test_vocabulary_created_correctly\n",
        "\n",
        "\n",
        "vocabulary: dict[str, int] = ...\n",
        "\n",
        "test_vocabulary_created_correctly(vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of `vocabulary`: 1130\n",
            "Samples:  [('!', 0), ('\"', 1), (\"'\", 2), ('(', 3), (')', 4), (',', 5), ('--', 6), ('.', 7), (':', 8), (';', 9)]\n"
          ]
        }
      ],
      "source": [
        "print(\"Length of `vocabulary`:\", len(vocabulary))\n",
        "print(\"Samples: \", list(vocabulary.items())[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Exercise: Convert Text to IDs\n",
        "\n",
        "- `vocabulary`를 이용해서 `text`를 token IDs로 변환하는 함수 `encode`를 작성해보자."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llm_from_scratch.chapter_02.tests import test_encode_implemented_correctly\n",
        "\n",
        "\n",
        "def encode(text: str, vocabulary: dict[str, int]) -> list[int]: ...\n",
        "\n",
        "\n",
        "test_encode_implemented_correctly(encode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[53, 44, 149, 1003, 57, 38]\n"
          ]
        }
      ],
      "source": [
        "encoded = encode(\"I HAD always thought Jack Gisburn\", vocabulary)\n",
        "print(encoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Exercise: Convert Token IDs to Text\n",
        "\n",
        "- `vocabulary`를 이용해서 token IDs를 `text`로 변환하는 함수 `decode`를 작성해보자."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llm_from_scratch.chapter_02.tests import test_decode_implemented_correctly\n",
        "\n",
        "\n",
        "def decode(token_ids: list[int], vocabulary: dict[str, int]) -> str: ...\n",
        "\n",
        "\n",
        "test_decode_implemented_correctly(decode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I HAD always thought Jack Gisburn\n"
          ]
        }
      ],
      "source": [
        "decoded = decode(encoded, vocabulary)\n",
        "print(decoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Exercise: Implement Simple Tokenizer\n",
        "\n",
        "- 위에서 작성한 `encode`와 `decode`를 이용해 `SimpleTokenizerV1` 클래스를 작성해보자.\n",
        "- `encode` 메소드는 `text`를 token IDs로 변환하고, `decode` 메소드는 token IDs를 텍스트로 변환해야 함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llm_from_scratch.chapter_02.protocols import TokenizerProtocol\n",
        "from llm_from_scratch.chapter_02.tests import (\n",
        "    test_simple_tokenizer_v1_implemented_correctly,\n",
        ")\n",
        "\n",
        "\n",
        "class SimpleTokenizerV1(TokenizerProtocol):\n",
        "    def __init__(self, vocabulary: dict[str, int]) -> None: ...\n",
        "\n",
        "    def encode(self, text: str) -> list[int]: ...\n",
        "\n",
        "    def decode(self, token_ids: list[int]) -> str: ...\n",
        "\n",
        "\n",
        "test_simple_tokenizer_v1_implemented_correctly(SimpleTokenizerV1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[53, 44, 149, 1003, 57, 38] I HAD always thought Jack Gisburn\n"
          ]
        }
      ],
      "source": [
        "tokenizer = SimpleTokenizerV1(vocabulary)\n",
        "\n",
        "encoded = tokenizer.encode(\"I HAD always thought Jack Gisburn\")\n",
        "decoded = tokenizer.decode(encoded)\n",
        "\n",
        "print(encoded, decoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.4 Adding Special Context Tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Goals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The token is not in the vocabulary\n"
          ]
        }
      ],
      "source": [
        "tokenizer = SimpleTokenizerV1(vocabulary)\n",
        "\n",
        "try:\n",
        "    tokenizer.encode(\"Hello, do you like tea?\")\n",
        "except KeyError as e:\n",
        "    print(\"The token is not in the vocabulary\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- `SimpleTokenizerV1`은 vocabulary에 없는 단어를 처리할 수 없음\n",
        "- 이를 해결하기 위해 special context tokens을 이용할 것임\n",
        "- 또한 special context token을 통해 모델이 문맥이나 다른 정보들을 이해할 수 있도록 하는 방법을 알아볼 것임\n",
        "  - e.g. 문장의 시작과 끝을 알리는 token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Exercise: Extend Vocabulary\n",
        "\n",
        "- 기존 `vocabulary`에 `<|endoftext|>`, `<|unk|>` token을 추가해보자.\n",
        "  - `<|endoftext|>`: 문장의 끝을 알리는 token\n",
        "  - `<|unk|>`: vocabulary에 없는 단어를 나타내는 token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llm_from_scratch.chapter_02.tests import test_extended_vocabulary_created_correctly\n",
        "\n",
        "vocabulary: dict[str, int] = ...\n",
        "\n",
        "test_extended_vocabulary_created_correctly(vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('yourself', 1129), ('<|endoftext|>', 1130), ('<|unk|>', 1131)]\n"
          ]
        }
      ],
      "source": [
        "print(list(vocabulary.items())[-3:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Exercise: Implement Simple Tokenizer with Special Tokens\n",
        "\n",
        "- `SimpleTokenizerV2` 클래스를 작성해보자\n",
        "- `SimpleTokenizerV1`와 같은 형태를 따름\n",
        "- 다만 `SimpleTokenizerV2`는 `SimpleTokenizerV1`와 달리 vocabulary에 없는 토큰을 처리할 수 있어야 함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llm_from_scratch.chapter_02.tests import (\n",
        "    test_simple_tokenizer_v2_implemented_correctly,\n",
        ")\n",
        "\n",
        "\n",
        "class SimpleTokenizerV2:\n",
        "    def __init__(self, vocabulary: dict[str, int]) -> None: ...\n",
        "\n",
        "    def encode(self, text: str) -> list[int]: ...\n",
        "\n",
        "    def decode(self, token_ids: list[int]) -> str: ...\n",
        "\n",
        "\n",
        "test_simple_tokenizer_v2_implemented_correctly(SimpleTokenizerV2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n",
            "<|unk|> , do you like tea ? <|endoftext|> In the sunlit terraces of the <|unk|> .\n"
          ]
        }
      ],
      "source": [
        "text = \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\"\n",
        "\n",
        "tokenizer = SimpleTokenizerV2(vocabulary)\n",
        "\n",
        "encoded = tokenizer.encode(text)\n",
        "print(encoded)\n",
        "\n",
        "decoded = tokenizer.decode(encoded)\n",
        "print(decoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Special Tokens in Real World\n",
        "\n",
        "- `BOS` (Beginning of Sequence)\n",
        "  - LLM에게 컨텐츠의 시작을 알리는 역할을 함\n",
        "- `EOS` (End of Sequence)\n",
        "  - LLM에게 컨텐츠의 끝을 알리는 역할을 함\n",
        "- `PAD` (Padding)\n",
        "  - LLM에게 sequence의 길이를 맞추기 위한 padding을 하는 역할을 함\n",
        "  - e.g. Batch 내에서 길이가 다른 sequence들을 padding하여 같은 길이로 만들어줌"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.5 Byte Pair Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example: `tiktoken`\n",
        "\n",
        "- `tiktoken`은 OpenAI에서 maintain하는 BPE tokenizer 라이브러리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]\n",
            "Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "text = (\n",
        "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\"\n",
        ")\n",
        "\n",
        "encoded = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "print(encoded)\n",
        "\n",
        "decoded = tokenizer.decode(encoded)\n",
        "\n",
        "print(decoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Observations\n",
        "\n",
        "- `<|endoftext|>`의 경우 상대적으로 큰 ID를 부여 받음\n",
        "  - GPT-2, GPT-3 등에서 쓰이는 이 tokenizer의 vocabulary 크기가 상대적으로 큰 50,257임을 알 수 있음.\n",
        "- \"someunknownPlace\"와 같이 vocabulary에 없는 단어도 처리할 수 있음\n",
        "  - `<|unk|>` token을 이용하지 않아도 됨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Algorithm (Brief)\n",
        "\n",
        "![](https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/11.webp)\n",
        "\n",
        "- OOV 토큰을 더 작은 subword나 character로 나누어서 모르는 토큰에 대해서도 핸들 가능하게 함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.6 Data Sampling with a Sliding Window"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Goal\n",
        "\n",
        "![](https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/12.webp)\n",
        "\n",
        "- Input-Target 쌍을 sliding window의 개념을 통해 생성할 것임\n",
        "  - Embedding을 만들기 위해 필요함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Input-Target Pairs\n",
        "\n",
        "- 위 figure에 묘사된 next-word prediction task는 아래 코드와 같이 표현될 수 있음"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[290] ----> 4920\n",
            "[290, 4920] ----> 2241\n",
            "[290, 4920, 2241] ----> 287\n",
            "[290, 4920, 2241, 287] ----> 257\n"
          ]
        }
      ],
      "source": [
        "sample_of_encoded: list[int] = tokenizer.encode(document)[50:]\n",
        "\n",
        "for i in range(1, 5):\n",
        "    context = sample_of_encoded[:i]\n",
        "    target_id = sample_of_encoded[i]\n",
        "    print(context, \"---->\", target_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 더 알아보기 쉽게 텍스트로 표현하면 아래와 같음"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " and ---->  established\n",
            " and established ---->  himself\n",
            " and established himself ---->  in\n",
            " and established himself in ---->  a\n"
          ]
        }
      ],
      "source": [
        "for i in range(1, 5):\n",
        "    context = sample_of_encoded[:i]\n",
        "    target_id = sample_of_encoded[i]\n",
        "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([target_id]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- (Personal Opinion -- Why there is no reason in the book? idk)\n",
        "  - 하지만 보통 input chunk와 target chunk의 사이즈를 같게 만듬\n",
        "    - Seq2Seq 모델의 경우 필수적임\n",
        "    - 같은 사이즈의 배치로 올려야 GPU 메모리를 더 효율적으로 이용할 수 있음\n",
        "    - 입력과 출력의 청크 사이즈가 같은 것이 구현이 쉬움\n",
        "- Input chunk와 target chunk의 사이즈가 같도록, input chunk를 window로 생각하고 target chunk를 input chunk로부터 1칸을 민 chunk로 고려할 것임\n",
        "\n",
        "![](https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/13.webp?123)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Exercise: Single Pair of Input Chunk and Target Chunk\n",
        "\n",
        "- token IDs가 주어졌을 때 `start_index`로부터 `context_size` 크기의 window `input_chunk`, `target_chunk`를 반환하는 함수 `input_chunk_and_target_chunk`을 구현해보자.\n",
        "  - `input_chunk`에는 input token들이 할당됨\n",
        "  - `target_chunk`에는 target token들이 포함된 token들이 할당됨\n",
        "    - input으로부터 1칸 shift 된 결과물"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llm_from_scratch.chapter_02.tests import (\n",
        "    test_input_chunk_and_target_chunk_created_correctly,\n",
        ")\n",
        "\n",
        "\n",
        "def input_chunk_and_target_chunk(\n",
        "    token_ids: list[int], start_index: int, context_size: int\n",
        ") -> tuple[list[int], list[int]]:\n",
        "    return input_chunk(token_ids, start_index, context_size), target_chunk(\n",
        "        token_ids, start_index, context_size\n",
        "    )\n",
        "\n",
        "\n",
        "def input_chunk(\n",
        "    token_ids: list[int], start_index: int, context_size: int\n",
        ") -> list[int]: ...\n",
        "\n",
        "\n",
        "def target_chunk(\n",
        "    token_ids: list[int], start_index: int, context_size: int\n",
        ") -> list[int]: ...\n",
        "\n",
        "\n",
        "test_input_chunk_and_target_chunk_created_correctly(input_chunk_and_target_chunk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " input_chunk: [290, 4920, 2241, 287]\n",
            "target_chunk:      [4920, 2241, 287, 257]\n"
          ]
        }
      ],
      "source": [
        "partial_encoded: list[int] = tokenizer.encode(document)[50:]\n",
        "context_size = 4\n",
        "\n",
        "_input_chunk, _target_chunk = input_chunk_and_target_chunk(\n",
        "    partial_encoded, 0, context_size\n",
        ")\n",
        "\n",
        "print(f\" input_chunk: {_input_chunk}\")\n",
        "print(f\"target_chunk:      {_target_chunk}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Exercise: Dataset\n",
        "\n",
        "- 텍스트와 tokenizer가 주어졌을 때 input chunk와 target chunk를 반환할 수 있는 torch Dataset `GPTDatasetV1`을 구현해보자."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Callable\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch import Tensor\n",
        "\n",
        "from llm_from_scratch.chapter_02.tests import test_gpt_dataset_v1\n",
        "\n",
        "\n",
        "class GPTDatasetV1(Dataset[tuple[Tensor, Tensor]]):\n",
        "    def __init__(\n",
        "        self,\n",
        "        text: str,\n",
        "        encode: Callable[[str], list[int]],\n",
        "        context_size: int,\n",
        "        stride: int,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        ...\n",
        "\n",
        "    def __len__(self): ...\n",
        "\n",
        "    def __getitem__(self, index: int | slice): ...\n",
        "\n",
        "\n",
        "test_gpt_dataset_v1(GPTDatasetV1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 아래 코드를 실행한 결과를 확인하여 위의 데이터셋이 잘 구성되었는지 확인할 수 있다.\n",
        "  - `Dataloader`를 통해서 데이터를 불러오는 코드이다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "def create_dataloader_v1(\n",
        "    text: str,\n",
        "    batch_size: int = 4,\n",
        "    context_size: int = 256,\n",
        "    stride: int = 128,\n",
        "    shuffle: bool = True,\n",
        "    drop_last: bool = True,\n",
        "):\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "    encode = tokenizer.encode\n",
        "\n",
        "    dataset = GPTDatasetV1(text, encode, context_size, stride)\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "    )\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "dataloader = create_dataloader_v1(\n",
        "    document, batch_size=1, context_size=4, stride=1, shuffle=False\n",
        ")\n",
        "\n",
        "data_iterator = iter(dataloader)\n",
        "\n",
        "first_batch = next(data_iterator)\n",
        "\n",
        "print(first_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- `first_batch`는 두 개의 텐서로 이루어져 있음\n",
        "  - 첫 번째 텐서는 input chunks, 두 번째 텐서는 target chunks를 나타냄"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
          ]
        }
      ],
      "source": [
        "second_batch = next(data_iterator)\n",
        "\n",
        "print(second_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- `second_batch`를 통해 stride의 역할을 확인할 수 있음\n",
        "  - 위에서 확인한 `first_batch`와 비교하여 한 칸이 밀려있는지 확인해보자.\n",
        "  - `context_size`와 동일하게 `stride`를 설정할 경우 배치들 간에 input이 overlap 되는 상황을 방지할 수 있음\n",
        "\n",
        "![](https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/14.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.7 Creating Token Embeddings\n",
        "\n",
        "### Embedding\n",
        "\n",
        "- Token IDs를 continuous vector space로 변환하는 것\n",
        "  - 각 토큰을 고유한 벡터로 매핑하는 것\n",
        "  - LLMs들이 back-propagation을 통해 학습할 수 있도록 함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Exercise: Torch Embedding\n",
        "\n",
        "- PyTorch의 `nn.Embedding`을 이용해서 embedding layer를 생성해보자.\n",
        "- Vocabulary의 크기는 6, token embedding의 차원은 3으로 생각"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llm_from_scratch.chapter_02.tests import test_simple_embedding\n",
        "\n",
        "\n",
        "torch.manual_seed(42)\n",
        "embedding: torch.nn.Embedding = ...\n",
        "\n",
        "test_simple_embedding(embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 1.9269,  1.4873, -0.4974],\n",
            "        [ 0.4396, -0.7581,  1.0783],\n",
            "        [ 0.8008,  1.6806,  0.3559],\n",
            "        [-0.6866,  0.6105,  1.3347],\n",
            "        [-0.2316,  0.0418, -0.2516],\n",
            "        [ 0.8599, -0.3097, -0.3957]], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "print(embedding.weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.6866,  0.6105,  1.3347]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(embedding(torch.tensor([3])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 위에서 생성한 embedding을 이용해서 token ID `3`에 상응하는 embedding vector를 구하면 위 embedding에서 4번째 row를 가져오는 것을 확인할 수 있다.\n",
        "  - 다시 말해서, embedding layer는 단순 lookup table로 이해할 수 있음"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.8 Encoding Word Positions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Positional Embeddings\n",
        "\n",
        "![](https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/17.webp)\n",
        "\n",
        "- 토큰의 위치 정보를 추가로 인코딩 하기 위해 \"Positional Embedding\"을 이용함\n",
        "  - \"Token embedding\"만 이용할 경우 토큰의 위치에 상관 없이 항상 같은 embedding이 할당됨\n",
        "    - 따라서 토큰의 위치 정보를 추가로 인코딩해야 함\n",
        "  - LLM이 문맥이나 토큰 사이의 관계를 더 잘 이해할 수 있도록 돕는 역할을 수행함\n",
        "- Types\n",
        "  - Absolute Positional Embeddings\n",
        "    - Input sequence의 각 위치에 대해 고유한 embedding을 더하는 방법\n",
        "    - ![](https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/18.webp)\n",
        "  - Relative Positional Embeddings\n",
        "    - 토큰 사이의 상대적인 위치나 거리를 인코딩하는 방법\n",
        "    - 다양한 길이의 sequence에 대해 일반화된 embedding을 제공할 수 있음\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Exercise: Positional Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataloader = create_dataloader_v1(\n",
        "    document, batch_size=8, context_size=4, stride=4, shuffle=False\n",
        ")\n",
        "\n",
        "data_iterator = iter(dataloader)\n",
        "\n",
        "first_batch = next(data_iterator)\n",
        "\n",
        "inputs, targets = first_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 우선 출력이 256차원이 되도록 embedding을 생성해보자.\n",
        "- Hint: `tokenizer`의 vocabulary size는 50257임"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llm_from_scratch.chapter_02.tests import test_token_embedding\n",
        "\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "embedding: torch.nn.Embedding = ...\n",
        "\n",
        "test_token_embedding(embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- `inputs`의 embedding을 구하자."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llm_from_scratch.chapter_02.tests import test_token_embeddings\n",
        "\n",
        "\n",
        "token_embeddings: torch.Tensor = ...\n",
        "\n",
        "test_token_embeddings(token_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ],
      "source": [
        "print(token_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Absolute position embeddings를 생성하자.\n",
        "  - Hint: Embedding layer의 row가 몇 개가 되어야 할까?\n",
        "  - Hint: Embedding layer의 출력 차원은 몇 차원이 되어야 할까?\n",
        "  - Hint: torch.arange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llm_from_scratch.chapter_02.tests import (\n",
        "    test_position_embedding,\n",
        "    test_position_embeddings,\n",
        ")\n",
        "\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "position_embedding: torch.nn.Embedding = ...\n",
        "\n",
        "test_position_embedding(position_embedding)\n",
        "\n",
        "position_embeddings: torch.Tensor = ...\n",
        "\n",
        "test_position_embeddings(position_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 1.9269,  1.4873,  0.9007,  ...,  0.5655,  0.5058,  0.2225],\n",
            "        [-0.6855,  0.5636, -1.5072,  ...,  0.4232, -0.3389,  0.5180],\n",
            "        [-1.3638,  0.1930, -0.6103,  ..., -1.6034, -0.4298,  0.5762],\n",
            "        [ 0.3444, -3.1016, -1.4587,  ...,  1.1085,  0.5544,  1.5818]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(position_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Input embeddings를 구해보자.\n",
        "  - Token embedding과 positional embedding을 결합하자."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llm_from_scratch.chapter_02.tests import test_input_embeddings\n",
        "\n",
        "\n",
        "input_embeddings: torch.Tensor = ...\n",
        "\n",
        "test_input_embeddings(input_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[ 1.4662e+00,  2.3496e+00,  8.7182e-01,  ..., -1.6724e-01,\n",
            "           2.8568e+00, -1.6840e-01],\n",
            "         [-1.2675e+00,  6.0994e-01, -1.7767e+00,  ..., -6.1660e-02,\n",
            "           6.2235e-01,  1.5896e+00],\n",
            "         [-1.9898e+00,  3.7392e-01, -6.4176e-01,  ..., -5.7213e-02,\n",
            "          -6.2085e-01, -1.3078e+00],\n",
            "         [ 2.0358e-01, -1.9663e+00, -1.2676e+00,  ..., -9.3234e-03,\n",
            "           1.4072e+00,  1.9885e+00]],\n",
            "\n",
            "        [[ 2.1036e+00,  1.6725e+00, -1.1920e+00,  ...,  2.4488e+00,\n",
            "           1.6690e+00,  3.6603e-01],\n",
            "         [-4.1414e-01,  9.4160e-01, -1.2794e+00,  ...,  2.0839e-01,\n",
            "          -1.7464e+00, -5.3957e-01],\n",
            "         [-5.2307e-01,  3.5381e-01,  1.4262e+00,  ..., -4.7622e-01,\n",
            "           3.5551e-01, -3.0002e-01],\n",
            "         [-6.8878e-01, -4.9592e+00, -1.2256e+00,  ...,  4.1810e-01,\n",
            "           3.7742e-01,  1.1978e+00]],\n",
            "\n",
            "        [[ 7.8653e-01,  3.2022e+00,  1.2175e+00,  ...,  4.2110e-01,\n",
            "           1.6269e+00,  1.5479e+00],\n",
            "         [ 5.7563e-01,  1.2872e+00, -2.7510e+00,  ...,  9.1495e-01,\n",
            "          -1.3827e+00,  1.8520e-01],\n",
            "         [-6.5592e-01, -1.1752e-01, -1.4967e+00,  ..., -1.1397e+00,\n",
            "          -9.6593e-01,  1.2796e+00],\n",
            "         [-1.1473e-01, -1.6040e+00, -9.6016e-01,  ...,  1.4863e+00,\n",
            "           9.9351e-01, -1.5015e+00]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.1708e+00,  9.3176e-01, -9.0565e-02,  ...,  1.3518e+00,\n",
            "           9.2445e-01,  8.7894e-01],\n",
            "         [-1.6252e+00,  1.6705e+00, -1.5923e+00,  ...,  1.2161e+00,\n",
            "          -1.5956e+00,  9.4337e-01],\n",
            "         [-1.0702e+00,  2.1916e+00, -5.1108e-01,  ..., -1.0721e+00,\n",
            "          -5.6991e-01,  2.5492e+00],\n",
            "         [-1.5389e-01, -3.1434e+00, -1.4312e+00,  ...,  3.2510e+00,\n",
            "           5.0581e-04,  2.6445e+00]],\n",
            "\n",
            "        [[ 2.2416e+00, -5.6392e-01,  1.3572e+00,  ...,  3.6971e-01,\n",
            "          -2.5300e-01,  1.5445e+00],\n",
            "         [ 3.2625e-02, -8.0610e-01, -2.3938e+00,  ..., -7.3971e-01,\n",
            "           2.1848e-01,  9.1309e-01],\n",
            "         [-1.4751e+00, -4.1772e-01, -1.2733e+00,  ..., -2.3160e+00,\n",
            "           1.0374e+00, -4.4352e-01],\n",
            "         [ 2.4940e-01, -3.3024e+00, -1.9059e+00,  ...,  4.5640e-01,\n",
            "          -2.6435e-01,  1.0080e+00]],\n",
            "\n",
            "        [[ 1.8155e+00,  8.7660e-01,  2.3772e-01,  ..., -1.4706e-01,\n",
            "           1.9730e+00, -7.9722e-01],\n",
            "         [-3.5975e-01,  6.9325e-01, -1.5194e+00,  ..., -1.5724e+00,\n",
            "          -9.0501e-03, -4.9607e-01],\n",
            "         [-1.5992e+00, -4.9761e-01, -1.3646e+00,  ..., -3.9207e+00,\n",
            "          -1.5839e+00, -4.5423e-01],\n",
            "         [ 2.1756e+00, -3.7175e+00, -2.0660e+00,  ..., -1.2807e+00,\n",
            "           1.2722e+00, -1.3592e-03]]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(input_embeddings)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
